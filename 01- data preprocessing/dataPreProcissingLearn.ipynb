{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "x = dataset.iloc[: , : -1].values\n",
    "y = dataset.iloc[: ,  -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country      3\n",
       "Age          9\n",
       "Salary       9\n",
       "Purchased    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take care of messing data (nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan , strategy='mean')\n",
    "imputer.fit(x[:,1:3]) # analyzes the data you provide and calculates the necessary statistics\n",
    "x[:,1:3] = imputer.transform(x[:,1:3]) # method uses the statistics computed during .fit() to replace the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the 3 classes (France , spain , germany) into 3 column usong *OneHotEncoder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder' ,OneHotEncoder() ,[0] )] , remainder=\"passthrough\")\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change yes and no to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splait dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y ,test_size=0.2 , random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featur scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between using `fit_transform` on `x_train` and `transform` on `x_test` when using `StandardScaler` lies in **why** and **when** the scaling parameters (mean and standard deviation) are computed and reused.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why `fit_transform` for `x_train`?**\n",
    "1. **`fit_transform` combines two steps:**\n",
    "   - `fit`: Computes the mean and standard deviation of the data (needed to scale the data).\n",
    "   - `transform`: Applies the scaling (subtracting the mean and dividing by the standard deviation).\n",
    "\n",
    "2. **Purpose in `x_train`:**\n",
    "   - The model should learn the scaling parameters (mean and standard deviation) **only from the training data** because it represents the real-world data distribution your model will encounter during training.\n",
    "   - Applying scaling on the same data ensures consistency and prevents information leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why `transform` for `x_test`?**\n",
    "1. **`transform` uses the scaling parameters** (mean and standard deviation) that were computed during the `fit` on `x_train`.\n",
    "2. **Purpose in `x_test`:**\n",
    "   - Test data should **not be used to calculate scaling parameters**, as the test set is supposed to simulate unseen data.\n",
    "   - Scaling the test set with the parameters learned from the training set ensures that the model evaluates data under the same conditions it was trained on.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Difference:**\n",
    "- `fit_transform`: **Fits** the scaler on data (computes mean & std) and then **transforms** it. Use this on the **training data**.\n",
    "- `transform`: Reuses the previously computed parameters to scale new data (e.g., test data or new unseen data).\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Context Example:**\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "import numpy as np\n",
    "data = np.array([[1, 2, 3, 100], \n",
    "                 [4, 5, 6, 200], \n",
    "                 [7, 8, 9, 300], \n",
    "                 [10, 11, 12, 400]])\n",
    "\n",
    "# Splitting data\n",
    "x_train, x_test = train_test_split(data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Applying StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform training data\n",
    "x_train_scaled = scaler.fit_transform(x_train[:, 3:])  # Scale only the 4th column (index 3)\n",
    "\n",
    "# Transform test data using the training parameters\n",
    "x_test_scaled = scaler.transform(x_test[:, 3:])\n",
    "```\n",
    "\n",
    "Here:\n",
    "- `fit_transform` is used for `x_train` to compute scaling parameters (mean, std) from the training data.\n",
    "- `transform` is used for `x_test` to apply the same scaling based on those parameters, ensuring consistency.\n",
    "\n",
    "Let me know if youâ€™d like more examples or explanations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train=sc.fit_transform(x_train[: , 3:])\n",
    "x_test = sc.transform(x_test[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19159184 -1.07812594]\n",
      " [-0.01411729 -0.07013168]\n",
      " [ 0.56670851  0.63356243]\n",
      " [-0.30453019 -0.30786617]\n",
      " [-1.90180114 -1.42046362]\n",
      " [ 1.14753431  1.23265336]\n",
      " [ 1.43794721  1.57499104]\n",
      " [-0.74014954 -0.56461943]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.46618179 -0.9069571 ]\n",
      " [-0.44973664  0.20564034]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
